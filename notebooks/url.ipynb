{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e82a0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================\n",
    "# URL PHISHING DETECTION — CONFERENCE-GRADE NOTEBOOK\n",
    "# ==============================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import joblib\n",
    "import requests\n",
    "import tldextract\n",
    "from bs4 import BeautifulSoup\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    confusion_matrix\n",
    ")\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e518fe49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_digits(s): \n",
    "    return sum(c.isdigit() for c in s)\n",
    "\n",
    "def count_letters(s): \n",
    "    return sum(c.isalpha() for c in s)\n",
    "\n",
    "def count_special(s): \n",
    "    return sum(not c.isalnum() for c in s)\n",
    "\n",
    "def get_html(url, timeout=7):\n",
    "    try:\n",
    "        r = requests.get(\n",
    "            url,\n",
    "            timeout=timeout,\n",
    "            headers={\"User-Agent\": \"Mozilla/5.0\"}\n",
    "        )\n",
    "        return r.text, r.status_code, r.history\n",
    "    except Exception:\n",
    "        return \"\", 0, []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1bffce28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(url: str, fetch_html=False) -> dict:\n",
    "    features = {}\n",
    "\n",
    "    features[\"URLLength\"] = len(url)\n",
    "    features[\"NoOfLettersInURL\"] = count_letters(url)\n",
    "    features[\"NoOfDegitsInURL\"] = count_digits(url)\n",
    "\n",
    "    features[\"LetterRatioInURL\"] = features[\"NoOfLettersInURL\"] / max(1, features[\"URLLength\"])\n",
    "    features[\"DegitRatioInURL\"] = features[\"NoOfDegitsInURL\"] / max(1, features[\"URLLength\"])\n",
    "\n",
    "    features[\"NoOfOtherSpecialCharsInURL\"] = count_special(url)\n",
    "    features[\"SpacialCharRatioInURL\"] = features[\"NoOfOtherSpecialCharsInURL\"] / max(1, features[\"URLLength\"])\n",
    "\n",
    "    ext = tldextract.extract(url)\n",
    "    domain_full = f\"{ext.domain}.{ext.suffix}\"\n",
    "\n",
    "    features[\"DomainLength\"] = len(domain_full)\n",
    "    features[\"TLDLength\"] = len(ext.suffix)\n",
    "    features[\"IsHTTPS\"] = 1 if url.startswith(\"https\") else 0\n",
    "    features[\"IsDomainIP\"] = 1 if re.match(r\"^\\d{1,3}(\\.\\d{1,3}){3}$\", ext.domain) else 0\n",
    "\n",
    "    soup = None\n",
    "    history = []\n",
    "\n",
    "    if fetch_html:\n",
    "        html, status, history = get_html(url)\n",
    "        soup = BeautifulSoup(html, \"html.parser\") if html else None\n",
    "\n",
    "    try:\n",
    "        title = soup.title.string.strip() if soup and soup.title else \"\"\n",
    "        features[\"HasTitle\"] = 1 if title else 0\n",
    "        features[\"Title\"] = len(title)\n",
    "    except:\n",
    "        features[\"HasTitle\"] = 0\n",
    "        features[\"Title\"] = 0\n",
    "\n",
    "    try:\n",
    "        desc = soup.find(\"meta\", {\"name\": \"description\"})\n",
    "        features[\"HasDescription\"] = 1 if desc and desc.get(\"content\") else 0\n",
    "    except:\n",
    "        features[\"HasDescription\"] = 0\n",
    "\n",
    "    features[\"NoOfJS\"] = len(soup.find_all(\"script\")) if soup else 0\n",
    "    features[\"NoOfCSS\"] = len(soup.find_all(\"link\", rel=\"stylesheet\")) if soup else 0\n",
    "    features[\"NoOfImage\"] = len(soup.find_all(\"img\")) if soup else 0\n",
    "    features[\"HasPasswordField\"] = 1 if soup and soup.find(\"input\", {\"type\": \"password\"}) else 0\n",
    "    features[\"NoOfURLRedirect\"] = len(history)\n",
    "\n",
    "    # Dataset-only features (kept zero for inference)\n",
    "    dataset_only = [\n",
    "        \"URLSimilarityIndex\",\"CharContinuationRate\",\"TLDLegitimateProb\",\n",
    "        \"URLCharProb\",\"NoOfSubDomain\",\"HasObfuscation\",\n",
    "        \"ObfuscationRatio\",\"NoOfEqualsInURL\"\n",
    "    ]\n",
    "    for f in dataset_only:\n",
    "        features[f] = 0\n",
    "\n",
    "    features[\"NoOfEqualsInURL\"] = url.count(\"=\")\n",
    "    features[\"NoOfQMarkInURL\"] = url.count(\"?\")\n",
    "\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b05cdfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Shape: (235795, 50)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/PhiUSIIL_Phishing_URL_Dataset.csv\")\n",
    "\n",
    "DROP_COLS = [\"FILENAME\", \"URL\", \"Domain\", \"TLD\", \"Title\"]\n",
    "df = df.drop(columns=DROP_COLS, errors=\"ignore\")\n",
    "df = df.fillna(0)\n",
    "\n",
    "X = df.drop(\"label\", axis=1)\n",
    "y = df[\"label\"]\n",
    "\n",
    "X = X.apply(pd.to_numeric, errors=\"coerce\").fillna(0)\n",
    "\n",
    "print(\"Dataset Shape:\", X.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "823688d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    stratify=y,\n",
    "    random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4865cf2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Accuracy: 1.0\n",
      "\n",
      "Confusion Matrix:\n",
      " [[20189     0]\n",
      " [    0 26970]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     20189\n",
      "           1       1.00      1.00      1.00     26970\n",
      "\n",
      "    accuracy                           1.00     47159\n",
      "   macro avg       1.00      1.00      1.00     47159\n",
      "weighted avg       1.00      1.00      1.00     47159\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    (\"model\", XGBClassifier(\n",
    "        n_estimators=350,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.09,\n",
    "        subsample=0.85,\n",
    "        colsample_bytree=0.85,\n",
    "        eval_metric=\"logloss\",\n",
    "        tree_method=\"hist\",\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "print(\"XGBoost Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d3227f50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost CV Accuracy: 1.0000 ± 0.0000\n"
     ]
    }
   ],
   "source": [
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cv_scores = []\n",
    "\n",
    "for tr, val in skf.split(X, y):\n",
    "    pipeline.fit(X.iloc[tr], y.iloc[tr])\n",
    "    preds = pipeline.predict(X.iloc[val])\n",
    "    cv_scores.append(accuracy_score(y.iloc[val], preds))\n",
    "\n",
    "print(\"XGBoost CV Accuracy: %.4f ± %.4f\" % (np.mean(cv_scores), np.std(cv_scores)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8846f148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logistic Regression Accuracy: 0.9998727708390763\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     20189\n",
      "           1       1.00      1.00      1.00     26970\n",
      "\n",
      "    accuracy                           1.00     47159\n",
      "   macro avg       1.00      1.00      1.00     47159\n",
      "weighted avg       1.00      1.00      1.00     47159\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "lr = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    LogisticRegression(max_iter=1000, n_jobs=-1)\n",
    ")\n",
    "\n",
    "lr.fit(X_train, y_train)\n",
    "lr_preds = lr.predict(X_test)\n",
    "\n",
    "print(\"\\nLogistic Regression Accuracy:\", accuracy_score(y_test, lr_preds))\n",
    "print(classification_report(y_test, lr_preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4e4af554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Random Forest Accuracy: 1.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     20189\n",
      "           1       1.00      1.00      1.00     26970\n",
      "\n",
      "    accuracy                           1.00     47159\n",
      "   macro avg       1.00      1.00      1.00     47159\n",
      "weighted avg       1.00      1.00      1.00     47159\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(\n",
    "    n_estimators=200,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "rf.fit(X_train, y_train)\n",
    "rf_preds = rf.predict(X_test)\n",
    "\n",
    "print(\"\\nRandom Forest Accuracy:\", accuracy_score(y_test, rf_preds))\n",
    "print(classification_report(y_test, rf_preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6760fbc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and feature list saved successfully.\n"
     ]
    }
   ],
   "source": [
    "joblib.dump(pipeline, \"urlphishing_model.pkl\")\n",
    "joblib.dump(list(X.columns), \"urlfeature_list.pkl\")\n",
    "\n",
    "print(\"Model and feature list saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9e210761",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_list = joblib.load(\"urlfeature_list.pkl\")\n",
    "\n",
    "def predict_url(url):\n",
    "    feats = extract_features(url, fetch_html=True)\n",
    "    df_test = pd.DataFrame([feats])\n",
    "\n",
    "    for col in feature_list:\n",
    "        if col not in df_test:\n",
    "            df_test[col] = 0\n",
    "\n",
    "    df_test = df_test[feature_list]\n",
    "    df_test = df_test.apply(pd.to_numeric, errors=\"coerce\").fillna(0)\n",
    "\n",
    "    pred = pipeline.predict(df_test)[0]\n",
    "    proba = pipeline.predict_proba(df_test)[0]\n",
    "\n",
    "    return (\"Phishing\" if pred else \"Legitimate\"), proba\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
